# Importation des biblioth√®ques n√©cessaires pour le d√©veloppement de l'application Streamlit et l'analyse de donn√©es
import streamlit as st  # Interface web interactive
import pandas as pd  # Manipulation de donn√©es avec DataFrames
import numpy as np  # Calculs num√©riques et g√©n√©ration de donn√©es al√©atoires
import seaborn as sns  # Visualisation statistique (ex: heatmap)
import matplotlib.pyplot as plt  # Graphiques classiques
import plotly.express as px  # Visualisations interactives
from sklearn.cluster import KMeans  # Algorithme de clustering non supervis√©
from sklearn.ensemble import RandomForestClassifier  # Classifieur supervis√© (arbres al√©atoires)
from sklearn.preprocessing import StandardScaler  # Normalisation des donn√©es
from mlxtend.frequent_patterns import apriori, association_rules  # Algorithmes de r√®gles d'association
import io  # Manipulation des fichiers en m√©moire (ex: PDF)
from reportlab.pdfgen import canvas  # G√©n√©ration de documents PDF
from reportlab.lib.pagesizes import letter  # Format de page pour PDF

# Configuration de la page Streamlit : titre de l'onglet du navigateur et mise en page large
st.set_page_config(page_title="Pr√©vention de l'abandon scolaire", layout="wide")

# Fonction pour charger et pr√©traiter les donn√©es OULAD
@st.cache_data  # Mise en cache pour √©viter de recharger inutilement les donn√©es
def load_oulad_data():
    assessments = pd.read_csv("assessments.csv")  # Chargement du fichier des √©valuations
    student_assessment = pd.read_csv("studentAssessment.csv")  # √âvaluations faites par les √©tudiants
    student_info = pd.read_csv("studentInfo.csv")  # Informations g√©n√©rales sur les √©tudiants

    # Fusion des fichiers CSV sur les cl√©s communes
    merged = student_assessment.merge(assessments, on='id_assessment', how='left')  # Fusion sur les ID d'√©valuation
    merged = merged.merge(student_info, on='id_student', how='left')  # Fusion avec les infos √©tudiants

    # Agr√©gation des donn√©es par √©tudiant
    df_grouped = merged.groupby('id_student').agg({
        'score': 'mean',  # Moyenne des scores
        'date_submitted': 'count',  # Nombre d'√©valuations soumises
        'studied_credits': 'mean',  # Moyenne des cr√©dits √©tudi√©s
        'age_band': 'first',  # Premi√®re tranche d'√¢ge rencontr√©e
        'gender': 'first',  # Sexe
        'region': 'first',  # R√©gion
        'highest_education': 'first',  # Niveau d'√©tude parental
        'disability': 'first',  # Handicap ou non
        'final_result': 'first'  # R√©sultat final
    }).reset_index()

    # Renommage des colonnes pour plus de clart√©
    df_grouped.rename(columns={
        'score': 'score_moyen',
        'date_submitted': 'nb_evaluations',
        'studied_credits': 'credits_etudies',
        'age_band': 'age',
        'gender': 'sexe',
        'highest_education': 'niveau_parental',
        'disability': 'handicap',
        'final_result': 'abandon'
    }, inplace=True)

    # Ajout de colonnes synth√©tiques simul√©es
    df_grouped['temps_moodle'] = np.random.uniform(0, 20, size=len(df_grouped))  # Temps pass√© sur la plateforme Moodle
    df_grouped['participation_forum'] = np.random.randint(0, 50, size=len(df_grouped))  # Messages sur le forum
    df_grouped['satisfaction'] = np.random.uniform(1, 5, size=len(df_grouped))  # Satisfaction √©tudiante simul√©e

    # Transformation de la variable 'abandon' en binaire : 1 = √©chec/retrait, 0 = r√©ussite
    df_grouped['abandon'] = df_grouped['abandon'].map(lambda x: 1 if x in ['Withdrawn', 'Fail'] else 0)

    return df_grouped  # Retourne le DataFrame pr√©par√©

# G√©n√®re des donn√©es synth√©tiques pour simuler d'autres sc√©narios
@st.cache_data
def generate_synthetic_data(n=1000):
    np.random.seed(42)  # Reproductibilit√© des donn√©es al√©atoires
    return pd.DataFrame({
        'age': np.random.randint(18, 50, n),  # Tranche d'√¢ge
        'sexe': np.random.choice(['M', 'F'], n),  # Sexe
        'region': np.random.choice(['Urbain', 'Rural'], n),  # Zone g√©ographique
        'niveau_parental': np.random.choice(['Secondaire', 'Sup√©rieur', 'Aucun'], n),  # √âducation parentale
        'note_moyenne': np.random.uniform(0, 100, n),  # Note moyenne simul√©e
        'taux_absent√©isme': np.random.uniform(0, 50, n),  # Absences simul√©es
        'temps_moodle': np.random.uniform(0, 20, n),  # Temps sur Moodle
        'participation_forum': np.random.randint(0, 50, n),  # Participation forum
        'satisfaction': np.random.uniform(1, 5, n),  # Satisfaction simul√©e
        'abandon': np.random.choice([0, 1], n, p=[0.8, 0.2])  # 80% non-abandon, 20% abandon
    })

# Pr√©traitement : gestion des valeurs manquantes et encodage des variables cat√©gorielles
def preprocess(df):
    df = df.fillna(df.mean(numeric_only=True))  # Remplir les valeurs num√©riques manquantes par la moyenne
    cat_cols = df.select_dtypes(include='object').columns  # Colonnes cat√©gorielles
    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)  # Encodage one-hot des variables cat√©gorielles
    return df

# Pipeline complet d‚Äôanalyse exploratoire et pr√©dictive
def analysis_pipeline(df, label, pdf_key):
    df = preprocess(df)  # Pr√©traitement des donn√©es

    # üîç Analyse exploratoire
    st.subheader("Analyse Exploratoire")
    st.plotly_chart(px.histogram(df, x=label, color='abandon'))  # Histogramme interactif

    # üî• Carte de corr√©lation
    fig, ax = plt.subplots()
    sns.heatmap(df.corr(), annot=False, cmap='coolwarm', ax=ax)  # Heatmap sans valeurs affich√©es
    st.pyplot(fig)

    # üì¶ Boxplots de comparaison par variable
    st.subheader("Boxplots")
    num_cols = df.select_dtypes(include=np.number).columns  # Colonnes num√©riques
    for col in num_cols:
        if col != 'abandon':  # Ne pas tracer pour la cible
            fig = px.box(df, y=col, color='abandon')  # Boxplot par classe d‚Äôabandon
            st.plotly_chart(fig)

    # üìä Clustering K-Means
    st.subheader("Clustering K-Means")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df.drop(['abandon'], axis=1, errors='ignore'))  # Normalisation des donn√©es
    df['cluster'] = KMeans(n_clusters=3, random_state=42).fit_predict(X_scaled)  # Application du clustering
    if label in df.columns:
        st.plotly_chart(px.scatter(df, x=label, y='temps_moodle', color='cluster'))  # Visualisation 2D

    # üå≤ Classification Random Forest
    st.subheader("Classification Random Forest")
    X = df.drop(['abandon', 'cluster'], axis=1, errors='ignore')  # Variables explicatives
    y = df['abandon']  # Cible
    clf = RandomForestClassifier().fit(X, y)  # Entra√Ænement du mod√®le
    st.success("Mod√®le entra√Æn√© avec succ√®s")

    # üìê R√®gles d'association (Apriori)
    st.subheader("R√®gles d'association")
    df_assoc = df.copy()
    for col in df_assoc.select_dtypes(include=['float64', 'int64']).columns:
        if df_assoc[col].nunique() >= 3:
            # Discr√©tisation des colonnes continues en 3 classes : bas, moyen, haut
            df_assoc[col] = pd.qcut(df_assoc[col], q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')
    df_assoc = pd.get_dummies(df_assoc)  # Encodage binaire
    df_assoc = df_assoc.loc[:, df_assoc.apply(lambda x: set(x.unique()).issubset({0, 1}))]  # Ne garder que les colonnes binaires

    # G√©n√©ration des r√®gles
    if not df_assoc.empty:
        freq_items = apriori(df_assoc, min_support=0.1, use_colnames=True)  # Objets fr√©quents
        rules = association_rules(freq_items, metric='confidence', min_threshold=0.6)  # R√®gles filtr√©es
        st.dataframe(rules[['antecedents', 'consequents', 'support', 'confidence']])  # Affichage
    else:
        st.info("Aucune r√®gle d'association trouv√©e.")

    # üß™ Simulation personnalis√©e pour un √©tudiant
    st.subheader("Simulation individuelle")
    input_data = {}
    for col in X.columns:
        if X[col].dtype in ['float64', 'int64']:
            input_data[col] = st.slider(f"{col}", float(X[col].min()), float(X[col].max()), float(X[col].mean()))
        else:
            input_data[col] = st.selectbox(f"{col}", X[col].unique())  # S√©lection pour variables cat√©gorielles
    input_df = pd.DataFrame([input_data])
    input_df = pd.get_dummies(input_df).reindex(columns=X.columns, fill_value=0)  # Aligner les colonnes

    risk = clf.predict_proba(input_df)[0][1]  # Pr√©diction de probabilit√© d‚Äôabandon
    st.metric(label="Risque d'abandon estim√©", value=f"{risk:.2%}")  # Affichage en pourcentage

    # üìÑ G√©n√©ration d‚Äôun rapport PDF t√©l√©chargeable
    st.subheader("T√©l√©chargement du rapport")
    if st.button("G√©n√©rer PDF", key=pdf_key):
        buffer = io.BytesIO()  # Buffer en m√©moire pour le PDF
        c = canvas.Canvas(buffer, pagesize=letter)  # D√©but du PDF
        c.drawString(100, 750, f"Risque d'abandon: {risk:.2%}")  # Titre
        y = 720
        for k, v in input_data.items():
            c.drawString(100, y, f"{k}: {v}")  # Affichage des param√®tres
            y -= 20
        c.save()  # Enregistrement du PDF
        buffer.seek(0)
        st.download_button("T√©l√©charger le rapport PDF", buffer, "rapport_abandon.pdf", "application/pdf")  # Bouton de t√©l√©chargement

# Interface principale de l‚Äôapplication
st.title("üéì Pr√©vention de l'abandon scolaire")

# Deux onglets : un pour les donn√©es r√©elles, un pour les donn√©es synth√©tiques
tabs = st.tabs(["Donn√©es r√©elles (OULAD)", "Donn√©es synth√©tiques"])

# Onglet 1 : donn√©es r√©elles
with tabs[0]:
    df_oulad = load_oulad_data()  # Chargement des donn√©es OULAD
    analysis_pipeline(df_oulad, label='score_moyen', pdf_key="pdf_oulad")  # Pipeline d‚Äôanalyse

# Onglet 2 : donn√©es simul√©es
with tabs[1]:
    df_synth = generate_synthetic_data()  # Cr√©ation des donn√©es synth√©tiques
    analysis_pipeline(df_synth, label='note_moyenne', pdf_key="pdf_synth")  # Pipeline d‚Äôanalyse
