# ============================================
# Projet : Pr√©vention de l'abandon scolaire avec Streamlit
# Auteur : KOMOSSI Sosso
# ============================================

# === Importation des biblioth√®ques ===
import streamlit as st  # Biblioth√®que pour cr√©er des applications web interactives
import pandas as pd  # Manipulation et analyse de donn√©es sous forme de DataFrames
import numpy as np  # Calculs num√©riques et g√©n√©ration de nombres al√©atoires
import seaborn as sns  # Visualisation statistique avanc√©e (heatmaps, distributions)
import matplotlib.pyplot as plt  # Cr√©ation de graphiques et visualisations
import plotly.express as px  # Graphiques interactifs modernes
from sklearn.cluster import KMeans  # Algorithme de clustering non supervis√©
from sklearn.ensemble import RandomForestClassifier  # Algorithme de classification supervis√©e
from sklearn.model_selection import train_test_split  # Division des donn√©es en ensembles d'entra√Ænement et de test
from sklearn.preprocessing import StandardScaler  # Normalisation et standardisation des donn√©es
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # √âvaluation des performances de classification
from mlxtend.frequent_patterns import apriori, association_rules  # Extraction de r√®gles d'association
import io  # Gestion des flux de donn√©es en m√©moire
from reportlab.lib.pagesizes import letter  # Format de page pour les documents PDF
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle  # √âl√©ments de mise en page PDF
from reportlab.lib import colors  # Palette de couleurs pour les documents PDF
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle  # Styles de formatage pour les paragraphes PDF

# === Configuration de la page Streamlit ===
st.set_page_config(page_title="Pr√©vention de l'abandon scolaire", layout="wide")  # Configuration du titre et de la largeur de la page

# === Chargement des donn√©es r√©elles OULAD ===
@st.cache_data  # Mise en cache des donn√©es pour √©viter le rechargement √† chaque ex√©cution
def load_oulad_data():  # Fonction pour charger et pr√©traiter les donn√©es OULAD
    assessments = pd.read_csv("assessments.csv")  # Lecture du fichier CSV contenant les informations sur les √©valuations
    student_assessment = pd.read_csv("studentAssessment.csv")  # Lecture du fichier CSV contenant les notes des √©tudiants
    student_info = pd.read_csv("studentInfo.csv")  # Lecture du fichier CSV contenant les informations personnelles des √©tudiants

    merged = student_assessment.merge(assessments, on='id_assessment')  # Fusion des donn√©es d'√©valuation avec les informations d'√©valuation
    merged = merged.merge(student_info, on='id_student')  # Fusion avec les informations personnelles des √©tudiants

    df = merged.groupby('id_student').agg({  # Regroupement des donn√©es par √©tudiant et agr√©gation
        'score': 'mean',  # Calcul du score moyen par √©tudiant
        'date_submitted': 'count',  # Comptage du nombre d'√©valuations soumises
        'studied_credits': 'mean',  # Calcul de la moyenne des cr√©dits √©tudi√©s
        'age_band': 'first',  # R√©cup√©ration de la premi√®re valeur de la tranche d'√¢ge
        'gender': 'first',  # R√©cup√©ration de la premi√®re valeur du genre
        'region': 'first',  # R√©cup√©ration de la premi√®re valeur de la r√©gion
        'highest_education': 'first',  # R√©cup√©ration de la premi√®re valeur du niveau d'√©ducation
        'disability': 'first',  # R√©cup√©ration de la premi√®re valeur du statut de handicap
        'final_result': 'first'  # R√©cup√©ration de la premi√®re valeur du r√©sultat final
    }).reset_index()  # R√©initialisation de l'index apr√®s le regroupement

    df.rename(columns={  # Renommage des colonnes pour une meilleure lisibilit√© en fran√ßais
        'score': 'score_moyen',  # Renommage de 'score' en 'score_moyen'
        'date_submitted': 'nb_evaluations',  # Renommage de 'date_submitted' en 'nb_evaluations'
        'studied_credits': 'credits_etudies',  # Renommage de 'studied_credits' en 'credits_etudies'
        'age_band': 'age',  # Renommage de 'age_band' en 'age'
        'gender': 'sexe',  # Renommage de 'gender' en 'sexe'
        'highest_education': 'niveau_parental',  # Renommage de 'highest_education' en 'niveau_parental'
        'disability': 'handicap',  # Renommage de 'disability' en 'handicap'
        'final_result': 'abandon'  # Renommage de 'final_result' en 'abandon'
    }, inplace=True)  # Modification directe du DataFrame original

    df['temps_moodle'] = np.random.uniform(0, 20, size=len(df))  # G√©n√©ration al√©atoire du temps pass√© sur la plateforme Moodle
    df['participation_forum'] = np.random.randint(0, 50, size=len(df))  # G√©n√©ration al√©atoire du nombre de participations aux forums
    df['satisfaction'] = np.random.uniform(1, 5, size=len(df))  # G√©n√©ration al√©atoire du niveau de satisfaction (√©chelle 1-5)
    df['abandon'] = df['abandon'].map(lambda x: 1 if x in ['Withdrawn', 'Fail'] else 0)  # Conversion de la variable d'abandon en binaire (1=abandon, 0=r√©ussite)

    return df  # Retour du DataFrame pr√©trait√©

# === G√©n√©ration de donn√©es synth√©tiques ===
@st.cache_data  # Mise en cache des donn√©es synth√©tiques pour √©viter la r√©g√©n√©ration
def generate_synthetic_data(n=1000):  # Fonction pour g√©n√©rer des donn√©es synth√©tiques
    np.random.seed(42)  # Fixation de la graine al√©atoire pour assurer la reproductibilit√©
    return pd.DataFrame({  # Cr√©ation d'un DataFrame avec des donn√©es synth√©tiques
        'age': np.random.randint(18, 50, n),  # G√©n√©ration d'√¢ges al√©atoires entre 18 et 50 ans
        'sexe': np.random.choice(['M', 'F'], n),  # G√©n√©ration al√©atoire du sexe (Masculin ou F√©minin)
        'region': np.random.choice(['Urbain', 'Rural'], n),  # G√©n√©ration al√©atoire de la r√©gion (Urbain ou Rural)
        'niveau_parental': np.random.choice(['Secondaire', 'Sup√©rieur', 'Aucun'], n),  # G√©n√©ration al√©atoire du niveau d'√©ducation parental
        'note_moyenne': np.random.uniform(0, 100, n),  # G√©n√©ration de notes moyennes al√©atoires entre 0 et 100
        'taux_absenteisme': np.random.uniform(0, 50, n),  # G√©n√©ration de taux d'absent√©isme al√©atoires entre 0 et 50%
        'temps_moodle': np.random.uniform(0, 20, n),  # G√©n√©ration du temps pass√© sur Moodle (0-20 heures)
        'participation_forum': np.random.randint(0, 50, n),  # G√©n√©ration du nombre de participations aux forums (0-50)
        'satisfaction': np.random.uniform(1, 5, n),  # G√©n√©ration du niveau de satisfaction (1-5)
        'abandon': np.random.choice([0, 1], n, p=[0.8, 0.2])  # G√©n√©ration de la variable d'abandon avec 20% de probabilit√© d'abandon
    })

# === Pr√©traitement des donn√©es ===
def preprocess(df):  # Fonction de pr√©traitement des donn√©es
    df = df.copy()  # Cr√©ation d'une copie du DataFrame pour √©viter les modifications sur l'original
    df = df.fillna(df.mean(numeric_only=True))  # Remplacement des valeurs manquantes par la moyenne pour les colonnes num√©riques
    cat_cols = df.select_dtypes(include='object').columns  # Identification des colonnes cat√©gorielles (type object)
    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)  # Encodage one-hot des variables cat√©gorielles avec suppression de la premi√®re modalit√©
    return df  # Retour du DataFrame pr√©trait√©

# === Pipeline d'analyse ===
def analysis_pipeline(df, label, pdf_key):  # Pipeline principal d'analyse des donn√©es
    df = preprocess(df)  # Pr√©traitement des donn√©es avant analyse

    st.subheader("1. Statistiques Descriptives")  # Affichage du titre de la section des statistiques descriptives
    st.dataframe(df.describe())  # Affichage des statistiques descriptives (moyenne, m√©diane, √©cart-type, etc.)

    st.subheader("2. Histogrammes")  # Affichage du titre de la section des histogrammes
    for col in ['score_moyen', 'note_moyenne', 'satisfaction', 'temps_moodle', 'participation_forum']:  # Boucle sur les colonnes importantes √† visualiser
        if col in df.columns:  # V√©rification de l'existence de la colonne dans le DataFrame
            st.plotly_chart(px.histogram(df, x=col, color=df['abandon'].astype(str)))  # Cr√©ation d'un histogramme color√© selon le statut d'abandon

    st.subheader("3. Matrice de Corr√©lation")  # Affichage du titre de la section de corr√©lation
    fig, ax = plt.subplots()  # Cr√©ation d'une nouvelle figure matplotlib
    sns.heatmap(df.corr(), annot=False, cmap="coolwarm", ax=ax)  # Cr√©ation d'une heatmap des corr√©lations entre variables
    st.pyplot(fig)  # Affichage de la figure dans Streamlit

    st.subheader("4. Boxplots")  # Affichage du titre de la section des boxplots
    for col in df.select_dtypes(include=np.number).columns:  # Boucle sur toutes les colonnes num√©riques
        if col != 'abandon':  # Exclusion de la variable cible pour √©viter la redondance
            st.plotly_chart(px.box(df, y=col, color=df['abandon'].astype(str)))  # Cr√©ation d'un boxplot color√© selon le statut d'abandon

    st.subheader("5. Clustering K-Means")  # Affichage du titre de la section de clustering
    X_clust = StandardScaler().fit_transform(df.drop(['abandon'], axis=1, errors='ignore'))  # Standardisation des donn√©es pour le clustering
    df['cluster'] = KMeans(n_clusters=3, random_state=42).fit_predict(X_clust)  # Application de l'algorithme K-Means avec 3 clusters
    st.plotly_chart(px.scatter(df, x=label, y='temps_moodle', color='cluster'))  # Visualisation des clusters dans un graphique en nuage de points

    st.subheader("6. Entra√Ænement du mod√®le Random Forest")  # Affichage du titre de la section d'entra√Ænement du mod√®le
    X = df.drop(['abandon', 'cluster'], axis=1, errors='ignore')  # D√©finition des variables explicatives (features)
    y = df['abandon']  # D√©finition de la variable cible (target)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # Division des donn√©es en ensembles d'entra√Ænement et de test
    clf = RandomForestClassifier()  # Cr√©ation d'une instance du classificateur Random Forest
    clf.fit(X_train, y_train)  # Entra√Ænement du mod√®le sur les donn√©es d'entra√Ænement
    st.success("Le mod√®le Random Forest a √©t√© entra√Æn√© avec succ√®s !")  # Message de confirmation de l'entra√Ænement

    st.subheader("7. Matrice de confusion")  # Affichage du titre de la section de la matrice de confusion
    y_pred = clf.predict(X_test)  # Pr√©diction sur l'ensemble de test
    cm = confusion_matrix(y_test, y_pred)  # Calcul de la matrice de confusion
    fig_cm, ax_cm = plt.subplots()  # Cr√©ation d'une nouvelle figure pour la matrice de confusion
    ConfusionMatrixDisplay(cm).plot(ax=ax_cm)  # Affichage de la matrice de confusion
    st.pyplot(fig_cm)  # Affichage de la figure dans Streamlit

    st.subheader("8. R√®gles d'association")  # Affichage du titre de la section des r√®gles d'association
    df_assoc = df.copy()  # Cr√©ation d'une copie du DataFrame pour l'analyse des r√®gles d'association
    for col in df_assoc.select_dtypes(include=['float64', 'int64']).columns:  # Boucle sur les colonnes num√©riques
        if df_assoc[col].nunique() > 2:  # Si la colonne a plus de 2 valeurs uniques
            df_assoc[col] = pd.qcut(df_assoc[col], 3, labels=["Low", "Medium", "High"], duplicates='drop')  # Discr√©tisation en 3 cat√©gories
    df_assoc = pd.get_dummies(df_assoc)  # Encodage one-hot de toutes les variables
    df_assoc = df_assoc.loc[:, df_assoc.apply(lambda x: set(x.unique()).issubset({0, 1}))]  # S√©lection des colonnes binaires uniquement
    freq_items = apriori(df_assoc, min_support=0.1, use_colnames=True)  # Extraction des itemsets fr√©quents avec support minimum de 0.1
    if not freq_items.empty:  # Si des itemsets fr√©quents ont √©t√© trouv√©s
        rules = association_rules(freq_items, metric="confidence", min_threshold=0.6)  # G√©n√©ration des r√®gles d'association avec confiance minimum de 0.6
        st.dataframe(rules[['antecedents', 'consequents', 'support', 'confidence']])  # Affichage des r√®gles trouv√©es
    else:  # Si aucun itemset fr√©quent n'a √©t√© trouv√©
        st.info("Aucune r√®gle d'association trouv√©e.")  # Message d'information

    st.subheader("9. Simulation individuelle")  # Affichage du titre de la section de simulation
    input_data = {}  # Dictionnaire pour stocker les donn√©es d'entr√©e de l'utilisateur
    for col in X.columns:  # Boucle sur toutes les colonnes de features
        if X[col].dtype in [np.float64, np.int64]:  # Si la colonne est num√©rique
            input_data[col] = st.slider(f"{col}", float(X[col].min()), float(X[col].max()), float(X[col].mean()))  # Cr√©ation d'un slider pour la saisie num√©rique
        else:  # Si la colonne est cat√©gorielle
            input_data[col] = st.selectbox(f"{col}", X[col].unique())  # Cr√©ation d'une bo√Æte de s√©lection pour la saisie cat√©gorielle

    input_df = pd.DataFrame([input_data])  # Cr√©ation d'un DataFrame avec les donn√©es saisies
    input_df = pd.get_dummies(input_df).reindex(columns=X.columns, fill_value=0)  # Encodage et alignement avec les colonnes du mod√®le
    risk = clf.predict_proba(input_df)[0][1]  # Calcul de la probabilit√© d'abandon pour l'√©tudiant simul√©
    st.metric(label="Risque d'abandon estim√©", value=f"{risk:.2%}")  # Affichage du risque estim√© sous forme de m√©trique

    st.subheader("10. Rapport PDF")  # Affichage du titre de la section de g√©n√©ration de rapport
    if st.button("G√©n√©rer PDF", key=pdf_key):  # Bouton pour d√©clencher la g√©n√©ration du PDF
        buffer = io.BytesIO()  # Cr√©ation d'un buffer en m√©moire pour le PDF
        doc = SimpleDocTemplate(buffer, pagesize=letter)  # Cr√©ation du document PDF avec format lettre
        styles = getSampleStyleSheet()  # R√©cup√©ration des styles par d√©faut
        elements = []  # Liste pour stocker les √©l√©ments du PDF

        title_style = ParagraphStyle(  # Cr√©ation d'un style personnalis√© pour le titre
            name='TitleStyle',  # Nom du style
            parent=styles['Heading1'],  # Style parent
            alignment=1,  # Alignement centr√©
            fontSize=18,  # Taille de police
            spaceAfter=20,  # Espace apr√®s le titre
            textColor=colors.darkblue  # Couleur du texte
        )
        elements.append(Paragraph("Rapport de Risque d'Abandon Scolaire", title_style))  # Ajout du titre au PDF

        table_data = [["Variable", "Valeur"]] + [[k, str(v)] for k, v in input_data.items()]  # Cr√©ation des donn√©es du tableau
        table = Table(table_data)  # Cr√©ation du tableau
        table.setStyle(TableStyle([  # Application du style au tableau
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#003366')),  # Couleur de fond de l'en-t√™te
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),  # Couleur du texte de l'en-t√™te
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),  # Bordures du tableau
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),  # Couleur de fond des cellules
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold')  # Police en gras pour l'en-t√™te
        ]))
        elements.append(table)  # Ajout du tableau au PDF
        elements.append(Spacer(1, 20))  # Ajout d'un espace vertical
        elements.append(Paragraph(f"<b>Risque estim√© d'abandon :</b> {risk:.2%}", styles['Normal']))  # Ajout du risque estim√©
        reco = "<b>Recommandation :</b> Suivi rapproch√© par le conseiller p√©dagogique" if risk >= 0.5 else "<b>Recommandation :</b> Maintenir les efforts actuels"  # D√©termination de la recommandation
        elements.append(Paragraph(reco, styles['Normal']))  # Ajout de la recommandation

        doc.build(elements)  # Construction du document PDF
        buffer.seek(0)  # Retour au d√©but du buffer
        st.download_button("T√©l√©charger le rapport PDF", buffer, "rapport_abandon.pdf", "application/pdf")  # Bouton de t√©l√©chargement du PDF

# === Interface principale ===
st.title("üéì Pr√©vention de l'abandon scolaire gr√¢ce au Data Mining")  # Titre principal de l'application
tabs = st.tabs(["Donn√©es r√©elles (OULAD)", "Donn√©es synth√©tiques"])  # Cr√©ation de deux onglets

with tabs[0]:  # Premier onglet pour les donn√©es r√©elles OULAD
    df_oulad = load_oulad_data()  # Chargement des donn√©es OULAD
    analysis_pipeline(df_oulad, label='score_moyen', pdf_key="pdf_oulad")  # Ex√©cution du pipeline d'analyse

with tabs[1]:  # Deuxi√®me onglet pour les donn√©es synth√©tiques
    df_synth = generate_synthetic_data()  # G√©n√©ration des donn√©es synth√©tiques
    analysis_pipeline(df_synth, label='note_moyenne', pdf_key="pdf_synth")  # Ex√©cution du pipeline d'analyse
