# ============================================
# Projet : Pr√©vention de l'abandon scolaire avec Streamlit
# Auteur : KOMOSSI Sosso
# Description : Application interactive pr√©dictive avec exploration, clustering, classification, simulation et rapport PDF
# ============================================

# === Importation des biblioth√®ques ===
import streamlit as st  # Biblioth√®que pour cr√©er une interface web interactive
import pandas as pd  # Manipulation efficace des donn√©es sous forme de DataFrames
import numpy as np  # Fonctions num√©riques avanc√©es et g√©n√©ration al√©atoire
import seaborn as sns  # Visualisation statistique (heatmaps, boxplots)
import matplotlib.pyplot as plt  # Cr√©ation de graphiques simples
import plotly.express as px  # Graphiques interactifs avanc√©s
from sklearn.cluster import KMeans  # Algorithme de clustering non supervis√©
from sklearn.ensemble import RandomForestClassifier  # Classifieur supervis√© robuste
from sklearn.model_selection import train_test_split  # S√©paration en ensemble d'entra√Ænement/test
from sklearn.preprocessing import StandardScaler  # Normalisation des donn√©es
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Matrice de confusion
from mlxtend.frequent_patterns import apriori, association_rules  # Extraction de r√®gles d'association
import io  # Gestion de fichiers en m√©moire
from reportlab.pdfgen import canvas  # G√©n√©ration de documents PDF
from reportlab.lib.pagesizes import letter  # Format de page standard pour PDF
from reportlab.platypus import Table, TableStyle, Paragraph, SimpleDocTemplate, Spacer  # Tableaux et paragraphes
from reportlab.lib import colors  # Couleurs pour PDF
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle  # Styles pour les paragraphes PDF

# === Configuration de la page Streamlit ===
st.set_page_config(page_title="Pr√©vention de l'abandon scolaire", layout="wide")  # Configuration du titre et layout de la page

# === Fonction de chargement des donn√©es OULAD (avec cache) ===
@st.cache_data  # D√©corateur pour mettre en cache les donn√©es et √©viter le rechargement

def load_oulad_data():  # Fonction pour charger et traiter les donn√©es OULAD
    assessments = pd.read_csv("assessments.csv")  # Lecture du fichier CSV des √©valuations
    student_assessment = pd.read_csv("studentAssessment.csv")  # Lecture du fichier CSV des notes des √©tudiants
    student_info = pd.read_csv("studentInfo.csv")  # Lecture du fichier CSV des informations des √©tudiants

    merged = student_assessment.merge(assessments, on='id_assessment')  # Fusion des √©valuations avec les notes par ID d'√©valuation
    merged = merged.merge(student_info, on='id_student')  # Fusion avec les informations √©tudiants par ID √©tudiant

    df = merged.groupby('id_student').agg({  # Regroupement par √©tudiant et agr√©gation des donn√©es
        'score': 'mean',  # Score moyen par √©tudiant
        'date_submitted': 'count',  # Nombre d'√©valuations soumises
        'studied_credits': 'mean',  # Moyenne des cr√©dits √©tudi√©s
        'age_band': 'first',  # Premi√®re valeur de la tranche d'√¢ge
        'gender': 'first',  # Premi√®re valeur du genre
        'region': 'first',  # Premi√®re valeur de la r√©gion
        'highest_education': 'first',  # Premi√®re valeur du niveau d'√©ducation
        'disability': 'first',  # Premi√®re valeur du handicap
        'final_result': 'first'  # Premi√®re valeur du r√©sultat final
    }).reset_index()  # R√©initialisation de l'index

    df.rename(columns={  # Renommage des colonnes en fran√ßais
        'score': 'score_moyen',  # Renommage de score en score_moyen
        'date_submitted': 'nb_evaluations',  # Renommage en nombre d'√©valuations
        'studied_credits': 'credits_etudies',  # Renommage en cr√©dits √©tudi√©s
        'age_band': 'age',  # Renommage en √¢ge
        'gender': 'sexe',  # Renommage en sexe
        'highest_education': 'niveau_parental',  # Renommage en niveau parental
        'disability': 'handicap',  # Renommage en handicap
        'final_result': 'abandon'  # Renommage en abandon
    }, inplace=True)  # Modification directe du DataFrame

    df['temps_moodle'] = np.random.uniform(0, 20, size=len(df))  # G√©n√©ration al√©atoire du temps pass√© sur Moodle
    df['participation_forum'] = np.random.randint(0, 50, size=len(df))  # G√©n√©ration al√©atoire de la participation aux forums
    df['satisfaction'] = np.random.uniform(1, 5, size=len(df))  # G√©n√©ration al√©atoire du niveau de satisfaction
    df['abandon'] = df['abandon'].map(lambda x: 1 if x in ['Withdrawn', 'Fail'] else 0)  # Conversion en variable binaire (1=abandon, 0=succ√®s)

    return df  # Retour du DataFrame trait√©

@st.cache_data  # D√©corateur pour mettre en cache les donn√©es synth√©tiques
def generate_synthetic_data(n=1000):  # Fonction pour g√©n√©rer des donn√©es synth√©tiques
    np.random.seed(42)  # Fixation de la graine al√©atoire pour la reproductibilit√©
    return pd.DataFrame({  # Cr√©ation d'un DataFrame avec des donn√©es al√©atoires
        'age': np.random.randint(18, 50, n),  # √Çge al√©atoire entre 18 et 50 ans
        'sexe': np.random.choice(['M', 'F'], n),  # Sexe al√©atoire (M ou F)
        'region': np.random.choice(['Urbain', 'Rural'], n),  # R√©gion al√©atoire (Urbain ou Rural)
        'niveau_parental': np.random.choice(['Secondaire', 'Sup√©rieur', 'Aucun'], n),  # Niveau d'√©ducation parental al√©atoire
        'note_moyenne': np.random.uniform(0, 100, n),  # Note moyenne al√©atoire entre 0 et 100
        'taux_absenteisme': np.random.uniform(0, 50, n),  # Taux d'absent√©isme al√©atoire entre 0 et 50%
        'temps_moodle': np.random.uniform(0, 20, n),  # Temps pass√© sur Moodle al√©atoire
        'participation_forum': np.random.randint(0, 50, n),  # Participation aux forums al√©atoire
        'satisfaction': np.random.uniform(1, 5, n),  # Satisfaction al√©atoire entre 1 et 5
        'abandon': np.random.choice([0, 1], n, p=[0.8, 0.2])  # Variable d'abandon avec 20% de probabilit√© d'abandon
    })

def preprocess(df):  # Fonction de pr√©processing des donn√©es
    df = df.copy()  # Cr√©ation d'une copie du DataFrame pour √©viter les modifications
    df = df.fillna(df.mean(numeric_only=True))  # Remplissage des valeurs manquantes par la moyenne
    cat_cols = df.select_dtypes(include='object').columns  # S√©lection des colonnes cat√©gorielles
    df = pd.get_dummies(df, columns=cat_cols, drop_first=True)  # Encodage one-hot des variables cat√©gorielles
    return df  # Retour du DataFrame pr√©process√©

def analysis_pipeline(df, label, pdf_key):  # Pipeline principal d'analyse des donn√©es
    df = preprocess(df)  # Pr√©processing des donn√©es

    st.subheader("1. Statistiques Descriptives")  # Affichage du titre de la section
    st.dataframe(df.describe())  # Affichage des statistiques descriptives

    st.subheader("2. Histogrammes pertinents")  # Affichage du titre de la section histogrammes
    for col in ['score_moyen', 'note_moyenne', 'satisfaction', 'temps_moodle', 'participation_forum']:  # Boucle sur les colonnes importantes
        if col in df.columns:  # V√©rification de l'existence de la colonne
            st.plotly_chart(px.histogram(df, x=col, color=df['abandon'].astype(str)))  # Cr√©ation d'un histogramme color√© par abandon

    st.subheader("3. Matrice de corr√©lation")  # Affichage du titre de la section corr√©lation
    fig, ax = plt.subplots()  # Cr√©ation d'une figure matplotlib
    sns.heatmap(df.corr(), annot=False, cmap="coolwarm", ax=ax)  # Cr√©ation d'une heatmap de corr√©lation
    st.pyplot(fig)  # Affichage de la figure dans Streamlit

    st.subheader("4. Boxplots par variable")  # Affichage du titre de la section boxplots
    for col in df.select_dtypes(include=np.number).columns:  # Boucle sur les colonnes num√©riques
        if col != 'abandon':  # Exclusion de la variable cible
            st.plotly_chart(px.box(df, y=col, color=df['abandon'].astype(str)))  # Cr√©ation d'un boxplot color√© par abandon

    st.subheader("5. Clustering KMeans")  # Affichage du titre de la section clustering
    X_clust = StandardScaler().fit_transform(df.drop(['abandon'], axis=1, errors='ignore'))  # Standardisation des donn√©es pour le clustering
    df['cluster'] = KMeans(n_clusters=3, random_state=42).fit_predict(X_clust)  # Application de K-means avec 3 clusters
    st.plotly_chart(px.scatter(df, x=label, y='temps_moodle', color='cluster'))  # Visualisation des clusters

    st.subheader("6. Classification Random Forest + matrice de confusion")  # Affichage du titre de la section classification
    X = df.drop(['abandon', 'cluster'], axis=1, errors='ignore')  # D√©finition des variables explicatives
    y = df['abandon']  # D√©finition de la variable cible
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # Division train/test
    clf = RandomForestClassifier().fit(X_train, y_train)  # Entra√Ænement du mod√®le Random Forest
    y_pred = clf.predict(X_test)  # Pr√©diction sur l'ensemble de test
    cm = confusion_matrix(y_test, y_pred)  # Calcul de la matrice de confusion
    fig_cm, ax_cm = plt.subplots()  # Cr√©ation d'une figure pour la matrice de confusion
    ConfusionMatrixDisplay(cm).plot(ax=ax_cm)  # Affichage de la matrice de confusion
    st.pyplot(fig_cm)  # Affichage dans Streamlit

    st.subheader("7. R√®gles d'association (Apriori)")  # Affichage du titre de la section r√®gles d'association
    df_assoc = df.copy()  # Copie du DataFrame pour les r√®gles d'association
    for col in df_assoc.select_dtypes(include=['float64', 'int64']).columns:  # Boucle sur les colonnes num√©riques
        if df_assoc[col].nunique() > 2:  # Si la colonne a plus de 2 valeurs uniques
            df_assoc[col] = pd.qcut(df_assoc[col], 3, labels=["Low", "Medium", "High"], duplicates='drop')  # Discr√©tisation en quartiles
    df_assoc = pd.get_dummies(df_assoc)  # Encodage one-hot
    df_assoc = df_assoc.loc[:, df_assoc.apply(lambda x: set(x.unique()).issubset({0, 1}))]  # S√©lection des colonnes binaires
    freq_items = apriori(df_assoc, min_support=0.1, use_colnames=True)  # Extraction des itemsets fr√©quents
    if not freq_items.empty:  # Si des itemsets fr√©quents sont trouv√©s
        rules = association_rules(freq_items, metric="confidence", min_threshold=0.6)  # G√©n√©ration des r√®gles d'association
        st.dataframe(rules[['antecedents', 'consequents', 'support', 'confidence']])  # Affichage des r√®gles
    else:  # Si aucun itemset fr√©quent n'est trouv√©
        st.info("Aucune r√®gle d'association trouv√©e.")  # Message d'information

    st.subheader("8. Simulation d'√©tudiant")  # Affichage du titre de la section simulation
    input_data = {}  # Dictionnaire pour stocker les donn√©es d'entr√©e
    for col in X.columns:  # Boucle sur toutes les colonnes de features
        if X[col].dtype in [np.float64, np.int64]:  # Si la colonne est num√©rique
            input_data[col] = st.slider(f"{col}", float(X[col].min()), float(X[col].max()), float(X[col].mean()))  # Slider pour saisie num√©rique
        else:  # Si la colonne est cat√©gorielle
            input_data[col] = st.selectbox(f"{col}", X[col].unique())  # Selectbox pour saisie cat√©gorielle

    input_df = pd.DataFrame([input_data])  # Cr√©ation d'un DataFrame avec les donn√©es d'entr√©e
    input_df = pd.get_dummies(input_df).reindex(columns=X.columns, fill_value=0)  # Encodage et alignement avec les colonnes du mod√®le
    risk = clf.predict_proba(input_df)[0][1]  # Pr√©diction de la probabilit√© d'abandon
    st.metric(label="Risque d'abandon estim√©", value=f"{risk:.2%}")  # Affichage du risque sous forme de m√©trique

    st.subheader("9. Rapport PDF")  # Affichage du titre de la section rapport PDF
    if st.button("G√©n√©rer PDF", key=pdf_key):  # Bouton pour g√©n√©rer le PDF
        buffer = io.BytesIO()  # Cr√©ation d'un buffer en m√©moire
        doc = SimpleDocTemplate(buffer, pagesize=letter)  # Cr√©ation du document PDF
        elements = []  # Liste pour stocker les √©l√©ments du PDF

        styles = getSampleStyleSheet()  # R√©cup√©ration des styles par d√©faut
        title_style = ParagraphStyle(  # Cr√©ation d'un style pour le titre
            name='TitleStyle', parent=styles['Heading1'], alignment=1, fontSize=18, spaceAfter=20, textColor=colors.darkblue  # D√©finition des propri√©t√©s du style
        )
        elements.append(Paragraph("Rapport de Risque d'Abandon Scolaire", title_style))  # Ajout du titre au PDF

        table_data = [["Variable", "Valeur"]] + [[k, str(v)] for k, v in input_data.items()]  # Cr√©ation des donn√©es du tableau
        table = Table(table_data, hAlign='LEFT')  # Cr√©ation du tableau
        table.setStyle(TableStyle([  # Application du style au tableau
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#003366')),  # Couleur de fond de l'en-t√™te
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),  # Couleur du texte de l'en-t√™te
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),  # Grille du tableau
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),  # Couleur de fond des cellules
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold')  # Police de l'en-t√™te
        ]))
        elements.append(table)  # Ajout du tableau au PDF
        elements.append(Spacer(1, 20))  # Ajout d'un espace

        elements.append(Paragraph(f"<b>Risque estim√© d'abandon :</b> {risk:.2%}", styles['Normal']))  # Ajout du risque estim√©
        reco = "<b>Recommandation :</b> Mettre en place un suivi p√©dagogique renforc√©" if risk >= 0.5 else "<b>Recommandation :</b> Maintenir la progression actuelle"  # D√©finition de la recommandation
        elements.append(Paragraph(reco, styles['Normal']))  # Ajout de la recommandation

        doc.build(elements)  # Construction du PDF
        buffer.seek(0)  # Retour au d√©but du buffer
        st.download_button("T√©l√©charger le rapport PDF", buffer, "rapport_abandon.pdf", "application/pdf")  # Bouton de t√©l√©chargement

# === Interface utilisateur principale ===
st.title("üéì Pr√©vention de l'abandon scolaire gr√¢ce au Data Mining")  # Titre principal de l'application
tabs = st.tabs(["Donn√©es r√©elles (OULAD)", "Donn√©es synth√©tiques"])  # Cr√©ation des onglets

with tabs[0]:  # Premier onglet pour les donn√©es OULAD
    df_oulad = load_oulad_data()  # Chargement des donn√©es OULAD
    analysis_pipeline(df_oulad, label='score_moyen', pdf_key="pdf_oulad")  # Ex√©cution du pipeline d'analyse

with tabs[1]:  # Deuxi√®me onglet pour les donn√©es synth√©tiques
    df_synth = generate_synthetic_data()  # G√©n√©ration des donn√©es synth√©tiques
    analysis_pipeline(df_synth, label='note_moyenne', pdf_key="pdf_synth")  # Ex√©cution du pipeline d'analyse
